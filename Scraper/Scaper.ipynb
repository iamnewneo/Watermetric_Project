{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just add to this list for each new player\n",
    "#player name : url\n",
    "query = \"https://www.google.co.in/search?q=%22yamuna+clean%22&tbm=nws&start=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#page = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.co.in/search?q=%22yamuna+clean%22&tbm=nws&start=0\n",
      "https://www.google.co.in/search?q=%22yamuna+clean%22&tbm=nws&start=10\n"
     ]
    }
   ],
   "source": [
    "#page = 0\n",
    "total = []\n",
    "results = []\n",
    "news_source_links = []\n",
    "for page in range(0,400,10):\n",
    "    print(query + str(page))\n",
    "    req  = requests.get(query + str(page), headers=headers)\n",
    "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
    "    news_stories = soup.findAll(\"div\", { \"class\" : \"_cnc\" })\n",
    "    for news in news_stories:\n",
    "        single_news= {}\n",
    "        single_news[\"news_link\"] = news.find(\"a\")[\"href\"]\n",
    "        single_news[\"news_title\"] = news.find(\"a\").get_text()\n",
    "        single_news[\"news_text\"] = news.find(\"div\", {\"class\": \"st\"}).get_text()\n",
    "        sourceAndTime = news.contents[1].get_text().split(\"-\")\n",
    "        single_news[\"news_source\"], single_news[\"news_time\"] = sourceAndTime[0], \"-\".join(sourceAndTime[1:])\n",
    "        results.append(single_news)\n",
    "        news_source_links.append(single_news[\"news_link\"])\n",
    "        #page = page + 10\n",
    "        time.sleep(1)\n",
    "        #print(single_news)\n",
    "        #print(\"\\n\")\n",
    "    #print(type(news_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#news_source_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run individual crawlers for websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def times_of_india(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"div\", class_=\"Normal\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    #print(article_header)\n",
    "    return article_text\n",
    "\n",
    "def the_hindu(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"div\", class_=\"articleLead\")\n",
    "    data=soup.find_all(\"p\", class_=\"body\")\n",
    "    #for element in article_header:\n",
    "        #article_text += element.get_text()\n",
    "    for element in data:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def daily_mail(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"p\", class_=\"mol-para-with-font\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def ndtv(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"div\", class_=\"ins_storybody\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def hindustan_times(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"p\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def indian_express(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"p\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def first_post(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"div\", class_=\"fullCont1\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def economic_times(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    article_header=soup.find_all(\"div\", class_=\"mod-economictimesarticletextwithadcpc mod-economictimesarticletext mod-articletext\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "\n",
    "def india_today(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    #article_header=soup.find_all(\"div\", class_=\"story_body_text\")\n",
    "    article_header=soup.findAll(\"div\", class_=\"right-story-container\")\n",
    "    for element in article_header:\n",
    "        article_text += element.get_text()\n",
    "    return article_text\n",
    "def other_site(src):\n",
    "    page  = requests.get(src, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(page.text,\"html.parser\")\n",
    "    article_text = \"\"\n",
    "    body_with_html=soup.find(\"body\")\n",
    "#     for element in article_header:\n",
    "#         article_text += element.get_text()\n",
    "    return body_with_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#times_of_india_text(\"http://timesofindia.indiatimes.com/city/delhi/SC-Detail-steps-taken-to-keep-Yamuna-clean/articleshow/15525134.cms\")\n",
    "#the_hindu(\"http://www.thehindu.com/data/last-drop-drinking-water-sipping-poison/article8557720.ece\")\n",
    "#daily_mail(\"http://www.dailymail.co.uk/indiahome/indianews/article-3136522/Yamuna-cleanup-committee-plans-pump-river-water-straight-Delhi-households.html\")\n",
    "#indian_express(\"http://indianexpress.com/article/cities/chandigarh/chandigarh-e-water-atms-to-be-set-up-at-22-spots-in-city-2807861/\")\n",
    "#other_site(\"http://indianexpress.com/article/cities/chandigarh/chandigarh-e-water-atms-to-be-set-up-at-22-spots-in-city-2807861/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    if \"timesofindia\" in result[\"news_link\"]:\n",
    "        article_text = times_of_india(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"thehindu\" in result[\"news_link\"]:\n",
    "        article_text = the_hindu(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"dailymail\" in result[\"news_link\"]:\n",
    "        article_text = daily_mail(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"indianexpress\" in result[\"news_link\"]:\n",
    "        article_text = indian_express(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"economictimes\" in result[\"news_link\"]:\n",
    "        article_text = economic_times(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"ndtv\" in result[\"news_link\"]:\n",
    "        article_text = ndtv(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"hindustantimes\" in result[\"news_link\"]:\n",
    "        article_text = hindustan_times(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"firstpost\" in result[\"news_link\"]:\n",
    "        article_text = first_post(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    elif \"indiatoday\" in result[\"news_link\"]:\n",
    "        article_text = india_today(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    else:\n",
    "        article_text = other_site(result[\"news_link\"])\n",
    "        result[\"article_text\"] = article_text\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Research paper link\n",
    "#http://link.springer.com/article/10.1007/s13201-011-0011-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "PIK = \"news_results.dat\"\n",
    "sys.setrecursionlimit(50000)\n",
    "#data = [\"A\", \"b\", \"C\", \"d\"]\n",
    "with open(PIK, \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Read Pickle File\n",
    "# with open(PIK, \"rb\") as f:\n",
    "#     news_results =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_text': '',\n",
       " 'news_link': 'http://swachhindia.ndtv.com/yamuna-clean-drive-gets-major-boost-by-ngt/',\n",
       " 'news_source': 'NDTV',\n",
       " 'news_text': 'The National Green Tribunal has directed the Agra Municipal Corporation to conduct a thorough clean-up of the Yamuna river bank, mainly the\\xa0...',\n",
       " 'news_time': '14-Jul-2016',\n",
       " 'news_title': 'Yamuna Clean-Up Drive Gets A Major Boost By NGT'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
